{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info=pd.read_csv('AsthmaFiles/patient_info.csv')\n",
    "smartwatch1=pd.read_csv('AsthmaFiles/smartwatch1.csv')\n",
    "smartwatch2=pd.read_csv('AsthmaFiles/smartwatch2.csv')\n",
    "smartwatch3=pd.read_csv('AsthmaFiles/smartwatch3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info[[\"user_key\"]+list(patient_info)[-11:-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_user_keys=[]\n",
    "for x in patient_info[[\"user_key\"]+list(patient_info)[-10:-2]].iterrows():\n",
    "    if x[1][\"pef_end_date\"] - x[1][\"pef_start_date\"] >= 50 and x[1][\"miband_end_date\"]!=\"NaN\":\n",
    "        list_of_user_keys.append(x[1][\"user_key\"])\n",
    "list_of_user_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peakflow_data(user_key):\n",
    "    peakflow_data = pd.read_csv(\"AsthmaFiles/peakflow.csv\")\n",
    "    peakflow_data = peakflow_data[peakflow_data[\"user_key\"]==user_key]\n",
    "    peakflow_data = peakflow_data[[\"date\",\"hour\",\"pef_max\"]]\n",
    "    return peakflow_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_weather(id,dates):\n",
    "    weather = pd.read_csv(\"AsthmaFiles/environment.csv\")\n",
    "    for_id = weather.loc[weather['user_key'] == id]\n",
    "    weather = for_id.loc[for_id['date'].isin(dates)]\n",
    "\n",
    "    return weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_flow_data = get_peakflow_data(list_of_user_keys[0])\n",
    "weather = pair_weather(list_of_user_keys[0],peak_flow_data[\"date\"])\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_catch(row,default,peak_flow_data,weather,x):\n",
    "    try:\n",
    "        return weather.loc[weather['date'] == row[\"date\"]].iloc[0][x]\n",
    "    except IndexError:\n",
    "        return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# seperate_for_key = smartwatch1.loc[smartwatch1[\"user_key\"]==list_of_user_keys[0]][[\"date\",\"time\",\"hr\"]]\n",
    "# seperate_for_date = seperate_for_key.loc[seperate_for_key[\"date\"]==1][[\"time\",\"hr\"]]\n",
    "# seperate_for_time = seperate_for_date.loc[seperate_for_date[\"time\"].str.startswith(\"08\")][\"hr\"]\n",
    "\n",
    "def seperate_for_key(user_key,smartwatch1,smartwatch2,smartwatch3):\n",
    "    seperate_for_key1 = smartwatch1.loc[smartwatch1[\"user_key\"]==user_key][[\"date\",\"time\",\"hr\"]]\n",
    "    seperate_for_key2 = smartwatch2.loc[smartwatch2[\"user_key\"]==user_key][[\"date\",\"time\",\"hr\"]]\n",
    "    seperate_for_key3 = smartwatch3.loc[smartwatch3[\"user_key\"]==user_key][[\"date\",\"time\",\"hr\"]]\n",
    "    frames = [seperate_for_key1, seperate_for_key2, seperate_for_key3]\n",
    "    return pd.concat(frames)\n",
    "\n",
    "def seperate_for_date(date,seperate_for_key):\n",
    "    return seperate_for_key.loc[seperate_for_key[\"date\"]==date][[\"time\",\"hr\"]]\n",
    "\n",
    "def seperate_for_time(time,seperate_for_date):\n",
    "    if len(str(time)) == 1:\n",
    "        time = \"0\" + str(time)\n",
    "    return seperate_for_date.loc[seperate_for_date[\"time\"].str.startswith(str(time))][\"hr\"].max()\n",
    "\n",
    "\n",
    "def confusing_method(row,key,sm1,sm2,sm3):\n",
    "    sk = seperate_for_key(key,sm1,sm2,sm3)\n",
    "    sd = seperate_for_date(row[\"date\"],sk)\n",
    "    return seperate_for_time(row[\"hour\"],sd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = \"NA\"\n",
    "for i in list_of_user_keys:\n",
    "    peak_flow_data = get_peakflow_data(i)\n",
    "    weather = pair_weather(i,peak_flow_data[\"date\"])\n",
    "    for x in list(weather.columns):\n",
    "        if x not in peak_flow_data.columns:\n",
    "            peak_flow_data[x] = peak_flow_data.apply(lambda row: try_catch(row,default,peak_flow_data,weather,x), axis = 1)\n",
    "    peak_flow_data[\"hr\"] = peak_flow_data.apply(lambda row: confusing_method(row,i,smartwatch1,smartwatch2,smartwatch3), axis = 1)\n",
    "    peak_flow_data.to_csv(f\"AsthmaFiles/{int(i)}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "\n",
    "dataset_190= pd.read_csv(\"AsthmaFiles/190.csv\")\n",
    "z = dataset_190[\"pef_max\"].max()\n",
    "# z = 575\n",
    "dataset_190 = dataset_190.drop([\"weed_pollen\",\"tree_pollen\",\"grass_pollen\"],axis=1)\n",
    "dataset_190=clearned_dataset_190 = clean_dataset(dataset_190)\n",
    "dataset_190\n",
    "y = dataset_190[\"pef_max\"].apply(lambda x: x/z)\n",
    "x = dataset_190.drop([\"pef_max\",\"date\",\"hour\",\"user_key\",\"co\",\"no\",\"no2\",\"o3\",\"so2\",\"pm2_5\",\"pm10\",\"nh3\"],axis=1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=74)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Assuming you have y_pred and y_test as NumPy arrays or Pandas Series\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'R-squared (R^2) Score: {r2}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 ('VEHackathon': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "db8df081959bc2e91e29711bcf4905a783752d3f89bf8bcd5589b22205a872f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
